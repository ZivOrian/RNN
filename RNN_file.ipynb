{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, autograd as grad\n",
    "from torch.utils.data import DataLoader, random_split, dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from AddClass import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embed_dim = 300\n",
    "word_num = 10_000\n",
    "num_layers = 2\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rnn(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Rnn, self).__init__()\n",
    "        self.rnnL1 = nn.RNN(input_size=Embed_dim ,hidden_size=1024,\n",
    "                nonlinearity= 'tanh' ,num_layers=num_layers, batch_first= True)\n",
    "        self.linL2 = nn.Linear(1024,5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "    def forward(self,x):\n",
    "        x,hidden =self.rnnL1(x)\n",
    "        x=self.relu(\n",
    "            self.linL2(x)\n",
    "        )\n",
    "        x=self.softmax(x)\n",
    "        \n",
    "        return(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset call section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['word'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dir_path = r\"C:\\Users\\orian\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Coding Files 2\\Datasets\"\n",
    "dataset = pd.read_csv(dir_path+r\"\\chat gpt reviews\\ChatGPT_Reviews.csv\")\n",
    "emb_dict = pd.read_csv(dir_path+r\"\\embedding dictionary\\dict(2).csv\")\n",
    "\n",
    "print(emb_dict.keys())\n",
    "\n",
    "# Create a dataset dictionary of reviews and ratings\n",
    "part_ds = [dataset['Review'], torch.tensor(dataset['Ratings'])]\n",
    "\n",
    "temp_part_ds = []\n",
    "# \n",
    "part_ds = list(zip(part_ds[0], part_ds[1]-1))\n",
    "\n",
    "train_len = round(len(part_ds)*0.9)\n",
    "train_ds_csv, test_ds_csv = random_split(\n",
    "    dataset=part_ds,lengths=[train_len, len(part_ds)-train_len])\n",
    "train_data = DataLoader(train_ds_csv, shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 300])\n"
     ]
    }
   ],
   "source": [
    "word_dict = emb_dict['word'].tolist()\n",
    "word_dict = set(word_dict)\n",
    "\n",
    "# Separate the words in the reviews and make sure they don't repeat themselves\n",
    "# (They are going to be added to the embeddings)\n",
    "review_set = {word for sentence in dataset['Review'].astype(str)\n",
    "               for word in sentence.split(' ')}\n",
    "\n",
    "combined_words = word_dict.union(review_set)\n",
    "\n",
    "# Dictionary to map words to indices\n",
    "word_to_index = {key: val for val, key in enumerate(combined_words)}\n",
    "embedding_layer = nn.Embedding(num_embeddings=len(word_to_index)\n",
    "                               , embedding_dim=Embed_dim)\n",
    "\n",
    "# Example of how to use the embedding layer that needs to be \n",
    "# deleted and moved to the feedforward section\n",
    "input_words = [\"suck\",\"the\",\"coconut\",\"juice\",\"out\",\"of\",\"the\",\"coconut\"]\n",
    "\n",
    "# Made to check if the embedding layer works for \n",
    "# the training dataset here - it does. See if it gets distorted further down the line\n",
    "tmp_random_input = list(train_data)[0][0]\n",
    "\n",
    "# Strip the mentioned values out of each word in the embedded sentence and then turn them into a word array\n",
    "tmp_random_input = tupleToArray(tmp_random_input)\n",
    "\n",
    "\n",
    "#input_indices = torch.LongTensor([word_to_index[word] for word in input_words])  # Convert words to indices\n",
    "input_indices = torch.LongTensor([word_to_index[word] for word in tmp_random_input])  # Convert words to indices\n",
    "embeddings = embedding_layer(input_indices)\n",
    "print(embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Epoch Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(rnn, train_ds, optimizer, dev, FILE_PATH, epoch_iter\n",
    "                 # Should add stuff that's required\n",
    "):\n",
    "    iter = 0\n",
    "\n",
    "    # Training loop part\n",
    "    for sample in train_ds:\n",
    "        rnn = rnn.to(dev) # Added due to errors (Should remain in the loop)\n",
    "        print(sample[0])\n",
    "        targetY = torch.nn.functional.one_hot(\n",
    "            torch.tensor(sample[1]), num_classes=5)\n",
    "        targetY = torch.tensor(targetY, dtype=torch.float32, requires_grad=True)\n",
    "        targetY = torch.squeeze(targetY).to(dev)\n",
    "        \n",
    "        # ---ForwardPropagation---\n",
    "        input_words = tupleToArray(sample[0])\n",
    "        embed_var = torch.LongTensor([word_to_index[word] for word in input_words])\n",
    "        embed_var = embedding_layer(embed_var).to(dev)\n",
    "        print(embed_var.shape)\n",
    "        print(embed_var)\n",
    "        pred = rnn.forward(embed_var).to(dev)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        # ---BackPropagation---\n",
    "        loss = loss_func(pred, targetY)\n",
    "        \"\"\" Use this link for the error:\n",
    "         https://discuss.pytorch.org/t/valueerror-expected-input-batch-size-324-to-match-target-batch-size-4/24498/4\n",
    "        \"\"\"\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter%100==0:\n",
    "            torch.save(rnn.cpu(), FILE_PATH)\n",
    "            grad_tmp = rnn.linL2.weight.grad\n",
    "            print(iter,\"({})\".format(epoch_iter+1), \" | \",grad_tmp.sum()**2/len(grad_tmp))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        iter+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Wonderfull App ðŸ«€',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orian\\AppData\\Local\\Temp\\ipykernel_15380\\464720283.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(sample[1]), num_classes=5)\n",
      "C:\\Users\\orian\\AppData\\Local\\Temp\\ipykernel_15380\\464720283.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targetY = torch.tensor(targetY, dtype=torch.float32, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 300])\n",
      "tensor([[ 7.5703e-01,  4.6492e-01,  4.3981e-01, -1.3652e+00, -1.2562e+00,\n",
      "         -2.3900e-01, -1.7695e+00,  8.9601e-01, -3.4862e-01,  1.0219e+00,\n",
      "         -5.2816e-01, -5.0088e-01,  2.5484e+00, -2.3730e-01,  1.0673e+00,\n",
      "         -1.0533e-02, -4.4679e-01,  7.6384e-02, -9.3777e-02, -7.3052e-01,\n",
      "         -4.5731e-01,  1.2461e+00, -3.0539e-01, -1.1649e+00, -6.7791e-01,\n",
      "         -5.0765e-01,  5.6716e-01,  5.1103e-01,  5.1356e-01, -7.7214e-01,\n",
      "         -1.4037e-01,  3.7745e-02, -4.1944e-01, -1.0599e+00,  6.4527e-01,\n",
      "          1.7221e+00, -2.7483e-01, -1.9844e-01, -1.2447e+00,  5.8512e-01,\n",
      "          1.7336e-01,  1.4955e+00, -9.7597e-01, -2.3586e-01, -4.9877e-01,\n",
      "         -1.1311e+00, -1.2403e+00,  2.1800e-01,  9.9735e-01, -8.1162e-01,\n",
      "          1.4817e-01, -8.3533e-01, -5.8241e-01,  8.7149e-01,  8.6926e-01,\n",
      "         -1.5881e+00,  1.2443e-02, -2.4655e-01,  1.8479e-01, -6.7236e-01,\n",
      "          8.8967e-01,  3.6975e-01,  2.9754e-02, -9.1278e-02, -5.7050e-01,\n",
      "          1.0228e+00,  1.1930e+00, -1.0060e+00,  1.4278e+00, -7.3278e-01,\n",
      "         -1.0152e+00, -2.1400e-01, -8.8864e-01,  1.4700e+00, -3.6418e-01,\n",
      "          5.9950e-02, -3.8898e-01, -1.9915e+00, -6.2574e-01,  2.0062e-01,\n",
      "         -1.2107e+00, -1.8625e+00, -9.9446e-02, -9.4308e-01, -5.8724e-01,\n",
      "          2.8104e-01, -1.7671e+00,  2.9867e-01, -1.7562e-01,  1.1247e+00,\n",
      "         -3.0343e-01,  1.1347e-01,  2.0467e+00,  3.4414e-02,  7.2733e-01,\n",
      "         -1.5440e+00, -6.3222e-01, -1.9478e-01,  2.3083e-01, -5.7984e-01,\n",
      "         -6.2371e-01, -8.9694e-01, -3.9532e-02,  3.4683e-01,  7.9166e-01,\n",
      "          4.1183e-01,  2.9126e-01, -1.1749e+00,  2.7176e-01, -1.2293e+00,\n",
      "         -9.1605e-01, -5.2541e-01,  3.8983e-01, -1.4238e+00,  1.8315e+00,\n",
      "          1.2003e+00,  8.9278e-01, -9.1078e-01,  1.2889e-01, -9.2766e-01,\n",
      "          4.7183e-01, -1.9623e+00,  8.9331e-01,  4.7881e-01,  3.4058e-01,\n",
      "         -1.7517e+00, -8.4817e-01, -3.9307e-01,  1.4611e+00, -1.3489e+00,\n",
      "         -6.3443e-01, -2.4307e+00,  1.6461e-02,  1.2873e+00, -4.9141e-01,\n",
      "          4.7161e-02,  9.4186e-01, -2.6610e-01,  1.2489e+00,  3.6795e-01,\n",
      "         -1.6937e-01,  4.3577e-01, -5.8294e-01,  1.7361e-01,  4.5260e-01,\n",
      "          5.5312e-01, -7.5272e-02, -7.3337e-02, -1.9629e-01, -1.0665e+00,\n",
      "          1.5250e+00, -1.0498e+00, -6.3229e-01, -1.5824e-01, -4.7797e-02,\n",
      "         -1.7176e-01, -1.6846e-01,  7.7064e-01,  1.0256e+00, -1.7096e+00,\n",
      "         -7.5000e-01, -1.3022e+00,  1.2700e+00, -1.2375e+00,  1.2074e-01,\n",
      "         -9.8165e-01, -1.4369e+00,  1.7740e+00,  3.9287e-01,  5.5795e-02,\n",
      "         -1.3608e+00,  3.5451e-01,  2.0434e+00, -1.3988e+00, -7.5347e-01,\n",
      "         -1.0411e+00, -6.7704e-01, -9.0820e-01,  1.3824e+00, -4.3364e-01,\n",
      "         -1.4196e-01, -2.7542e-01,  1.5100e-01,  2.2776e-01, -6.0912e-01,\n",
      "          2.3105e-01,  3.1734e-01, -1.4291e+00, -5.9964e-01, -3.0547e-01,\n",
      "         -1.4417e+00, -2.3741e-01, -1.7812e+00, -1.1300e+00,  9.9151e-01,\n",
      "          4.5084e-01,  1.0688e+00, -6.0783e-01,  1.4455e+00, -3.1657e-01,\n",
      "         -5.3928e-01,  5.6800e-01, -1.2974e+00, -1.7806e+00,  2.5639e+00,\n",
      "         -1.3804e+00, -5.4456e-01,  3.8859e-01, -1.4304e+00, -8.8889e-01,\n",
      "         -7.2070e-01,  6.1722e-01,  1.6463e+00,  5.2142e-01,  7.8369e-01,\n",
      "          1.7859e+00,  4.0548e-01,  1.1630e+00,  1.5122e+00, -2.0793e-01,\n",
      "         -3.6520e-01, -6.3418e-01, -2.1575e+00, -3.6311e-01, -6.0409e-01,\n",
      "         -1.1215e-01,  2.2001e+00,  5.3412e-01, -4.1040e-01,  6.2193e-01,\n",
      "          7.8200e-01,  1.1402e+00, -1.8100e+00, -3.9371e-01, -6.4944e-01,\n",
      "          1.6386e+00,  1.0769e+00, -1.5660e+00, -1.0180e+00,  9.8024e-01,\n",
      "         -9.7820e-01, -1.3163e+00,  8.6588e-01, -3.2111e-01, -4.1108e-01,\n",
      "         -1.1160e+00, -1.4480e-01,  1.5631e+00, -7.1186e-01, -1.4557e+00,\n",
      "          9.3943e-02, -1.0201e+00,  2.1626e-01, -1.7825e-01, -3.4160e-01,\n",
      "         -1.3106e+00,  4.2809e-01,  1.0155e+00,  1.4675e+00, -7.5229e-01,\n",
      "          8.0540e-01, -3.5662e-01,  5.4413e-01,  9.5400e-01, -1.0381e-01,\n",
      "          1.8123e+00, -2.8532e-01,  1.2372e+00,  1.1276e+00,  1.9560e+00,\n",
      "          8.0398e-01, -1.0321e+00,  1.0765e+00,  1.9173e+00,  4.2066e-01,\n",
      "         -6.1217e-01, -2.0221e+00,  8.6913e-02, -5.5769e-01, -5.3482e-01,\n",
      "         -4.3826e-01, -2.0606e-01, -1.8149e-01,  8.3015e-01,  1.2314e+00,\n",
      "          9.1319e-01, -3.4227e-01,  3.6162e-01,  4.9545e-01,  8.2337e-01,\n",
      "          2.5849e+00, -7.5291e-01, -8.0359e-01, -1.6669e-01, -5.9211e-01,\n",
      "         -1.0729e-01, -4.6103e-01, -1.2358e+00, -1.4281e+00,  9.3214e-01],\n",
      "        [ 1.0165e+00,  9.5195e-01,  3.4709e-01, -9.1071e-01,  2.9218e-01,\n",
      "          5.5040e-01,  1.4999e+00, -1.0190e+00, -1.7516e+00, -1.3980e+00,\n",
      "         -2.4406e-01,  1.0647e+00,  3.2613e-01, -2.5081e-01, -8.8210e-01,\n",
      "         -5.7680e-01,  7.0320e-01, -1.0083e+00,  9.5996e-01, -9.1381e-01,\n",
      "          1.4834e+00,  2.0666e+00,  6.9508e-01, -1.5930e+00, -3.5275e-01,\n",
      "          8.7995e-01, -1.9464e-01, -4.8370e-01,  9.1303e-01, -1.5548e+00,\n",
      "         -4.5946e-01, -1.0077e+00, -3.8767e-01,  7.6163e-01,  3.1914e-01,\n",
      "         -2.0699e+00, -1.9863e-01, -2.8452e-01, -5.3609e-01, -1.0756e+00,\n",
      "         -2.0227e+00, -3.5694e-02, -4.2352e-01,  9.3205e-01, -2.4349e+00,\n",
      "          9.0887e-01,  6.9049e-01,  1.9283e+00,  1.5654e+00, -8.7711e-01,\n",
      "         -5.9946e-01, -2.0390e-01,  2.2787e-01, -6.3891e-01,  1.3275e+00,\n",
      "          4.7237e-01, -9.3080e-01,  2.3750e-01, -8.1859e-01,  1.4463e+00,\n",
      "         -6.0073e-01,  1.5580e+00, -1.5701e+00, -5.0887e-01, -2.5321e+00,\n",
      "          9.1026e-01, -3.0258e-01,  3.4004e-01, -1.2728e+00,  8.7422e-01,\n",
      "          1.4973e+00,  1.5754e+00,  1.1972e+00, -9.2716e-01,  1.2760e+00,\n",
      "          1.5748e-01, -9.1134e-01,  2.0390e-01, -1.6666e+00, -2.3213e-01,\n",
      "         -1.0256e-01,  1.6191e-01, -7.4193e-01,  1.7790e-01, -1.4098e+00,\n",
      "         -3.5098e-01,  6.6851e-01,  1.1543e+00,  8.5733e-01,  4.4047e-02,\n",
      "          1.1080e+00, -3.6130e-01,  7.6340e-01, -1.6381e-01,  1.0181e+00,\n",
      "          1.8209e+00, -5.8916e-01,  7.4394e-01,  4.1374e-01,  4.6628e-01,\n",
      "         -2.4291e+00,  1.8995e+00,  7.6126e-01,  4.1710e-01,  1.8441e+00,\n",
      "          3.8880e-01,  1.0063e+00, -2.1863e-01,  3.1645e-01,  9.7964e-02,\n",
      "          1.2044e+00, -5.1330e-01,  4.7893e-01, -3.4587e-01, -1.4413e-01,\n",
      "          1.1270e+00, -1.2363e+00, -2.6893e+00, -8.7754e-01, -1.2240e-01,\n",
      "         -9.9502e-03,  1.0358e+00, -7.0621e-01, -4.0244e-01, -1.8311e+00,\n",
      "         -9.8100e-01,  2.6528e-02,  1.5897e+00,  2.0662e+00, -1.5504e-01,\n",
      "         -7.9856e-01, -7.4397e-02,  1.0592e+00, -1.8375e-01,  2.7217e+00,\n",
      "         -1.3125e+00, -1.5131e+00,  6.2091e-01, -1.4275e+00, -1.4793e-02,\n",
      "          1.8420e-01, -1.6698e-01,  4.4314e-01, -8.4091e-01, -1.6224e+00,\n",
      "          5.1224e-01, -1.4479e+00, -1.0302e+00,  1.0742e+00,  8.3042e-01,\n",
      "         -8.6135e-01,  7.2628e-01,  1.2410e+00, -8.2603e-01,  8.3097e-01,\n",
      "          5.5442e-01, -7.5068e-01,  2.2430e-01,  2.7134e-02,  5.3304e-01,\n",
      "         -3.7661e-01,  1.9433e+00, -1.2187e+00,  2.2957e+00, -1.3192e-01,\n",
      "         -2.4510e+00,  1.0662e+00,  2.2477e-01, -4.2565e-02,  8.0339e-01,\n",
      "         -2.0257e-01,  1.1817e+00,  5.9099e-01,  7.4498e-01, -6.0644e-01,\n",
      "          1.3359e+00, -1.5628e+00,  1.6936e+00, -9.3755e-01,  1.4154e+00,\n",
      "         -1.6598e+00, -2.8503e+00,  1.6304e+00,  7.8028e-01,  1.3119e-01,\n",
      "         -1.7844e-02, -7.4547e-01, -1.2133e+00, -1.9030e+00, -3.3931e-01,\n",
      "         -7.6315e-02, -1.5319e-02,  2.0618e+00,  1.7400e-01,  2.6873e+00,\n",
      "         -1.9646e-01,  1.7161e-01, -9.3632e-01, -1.8136e+00, -5.5636e-01,\n",
      "         -3.4971e-02, -5.6018e-01, -7.6361e-01,  1.3250e+00, -4.8109e-02,\n",
      "          8.9706e-01, -1.0175e+00, -2.4027e+00, -1.8567e+00,  4.0654e-02,\n",
      "          9.6859e-01, -3.4293e-01, -1.4962e+00, -1.0949e+00, -7.1568e-01,\n",
      "          3.9093e-02, -1.7704e+00,  8.9820e-01,  3.6065e-01, -7.7318e-01,\n",
      "          5.0885e-01,  2.1356e-01, -1.9597e+00, -8.8818e-01, -1.1371e-01,\n",
      "         -1.4796e+00, -3.1283e-01,  4.8533e-01, -1.1001e+00,  4.3202e-01,\n",
      "         -6.6298e-02, -9.2860e-02,  2.4911e-01, -1.8952e+00, -5.1139e-01,\n",
      "          1.2397e-01, -1.5058e+00, -6.6050e-01,  5.2346e-01,  7.6811e-01,\n",
      "          3.1667e+00, -1.9048e+00, -6.5887e-01, -2.0824e+00,  4.1602e-01,\n",
      "         -1.3831e+00,  2.2971e-01, -1.1938e-01,  1.8688e+00, -4.0062e-01,\n",
      "          4.0079e-01, -5.4931e-01, -4.4011e-01, -2.1604e+00,  6.3968e-01,\n",
      "          6.0439e-01, -9.0054e-02, -7.9656e-01,  5.2382e-01,  1.4263e-01,\n",
      "         -5.7221e-02, -9.8131e-01,  4.0683e-02,  3.7232e-01,  2.0148e+00,\n",
      "         -1.2508e+00, -8.7401e-01,  9.4792e-01, -9.7793e-03,  1.1355e+00,\n",
      "          1.4957e+00,  6.4545e-01, -2.1641e+00, -9.8872e-01,  1.3254e+00,\n",
      "          6.7472e-01,  1.2774e+00,  1.5881e-01, -1.3264e+00, -3.6289e-02,\n",
      "          1.3492e-01,  9.7873e-01, -1.6474e-01, -5.5714e-01, -1.6872e+00,\n",
      "         -2.1694e+00,  1.6793e+00, -9.9655e-01,  1.8658e-01, -1.3016e+00,\n",
      "         -8.9135e-01,  2.1880e+00, -2.2198e-01, -5.3086e-01, -1.2834e+00,\n",
      "          2.7152e-01, -6.4002e-01, -7.6717e-01, -4.0482e-01,  6.7231e-01],\n",
      "        [ 1.2326e+00, -4.5017e-01,  1.9527e-01,  2.3191e-02,  2.2736e+00,\n",
      "          1.2658e+00, -3.3570e-01, -6.2607e-01,  4.7432e-01,  2.3709e-01,\n",
      "          9.7041e-01,  1.2229e+00, -1.7472e+00, -6.0399e-02, -1.7351e+00,\n",
      "          5.7283e-01, -3.2971e-01, -5.5024e-01, -1.8621e-01,  8.5797e-01,\n",
      "         -3.6128e-01, -7.4879e-01,  7.0984e-01, -3.6838e-01, -1.2471e+00,\n",
      "          5.4877e-01, -2.2584e+00,  4.7511e-01, -3.4351e-01, -1.2855e+00,\n",
      "          1.1860e+00, -1.1744e+00, -2.2444e-02, -1.4922e+00, -1.2538e+00,\n",
      "         -1.7372e+00, -7.8581e-01,  2.0859e+00,  2.2215e+00, -1.8644e+00,\n",
      "         -6.2999e-01, -1.0960e+00,  1.1776e+00,  1.6493e+00, -1.7489e+00,\n",
      "          8.0948e-01,  4.2630e-01, -1.7807e+00,  2.3342e+00, -3.3004e-01,\n",
      "         -3.7377e-01,  8.9533e-01,  2.9255e-01, -4.2543e-02, -2.3360e-01,\n",
      "          1.8467e+00,  1.8204e-01, -1.3244e+00,  8.6905e-01, -5.2362e-01,\n",
      "         -2.5226e-01, -6.4465e-01,  1.2607e+00,  3.1025e-01, -7.3779e-01,\n",
      "          1.1926e+00,  1.4960e+00, -1.2766e+00,  7.2900e-01, -2.1446e-01,\n",
      "          2.9486e-01, -7.0180e-01, -1.8836e+00, -9.6054e-01, -3.4426e-01,\n",
      "          3.6206e-01,  5.1424e-01, -1.1270e+00, -1.1977e+00, -9.8127e-01,\n",
      "         -9.7988e-01, -9.9489e-01, -6.1904e-01,  2.1987e-01,  1.0673e+00,\n",
      "          2.9166e-01, -4.7543e-02, -8.3736e-01, -1.2439e+00, -1.0025e+00,\n",
      "          4.8114e-02,  8.1006e-01, -1.2181e-02, -4.4582e-01, -2.1091e-01,\n",
      "          7.8888e-01,  5.6415e-03, -1.3946e+00, -2.8846e-03, -1.9334e+00,\n",
      "          6.1130e-01,  8.7901e-01, -1.1302e-01,  8.5156e-01, -6.2627e-01,\n",
      "         -6.3207e-01,  7.3098e-01,  1.2069e+00, -6.4961e-02, -1.3924e+00,\n",
      "         -5.7750e-01, -4.5226e-01,  1.0353e+00, -5.6712e-02,  1.9488e+00,\n",
      "         -1.0668e-01,  3.3609e-01,  1.2292e+00,  9.8891e-01,  9.7401e-01,\n",
      "          1.8971e+00,  7.9780e-01,  1.6370e+00,  4.9297e-01,  4.7737e-01,\n",
      "         -7.1157e-01, -3.9984e-01, -3.9413e-01, -2.1452e+00,  6.3292e-02,\n",
      "         -3.4687e-01,  3.7201e-01, -3.7452e-01,  4.7144e-01,  1.5101e+00,\n",
      "         -6.5934e-01, -9.4535e-01, -1.6501e+00, -5.6093e-01,  2.0003e-01,\n",
      "         -6.8453e-01,  1.6094e+00, -7.1405e-02,  4.5028e-01,  3.3845e-01,\n",
      "          6.1002e-01, -9.7732e-01, -8.8117e-01, -1.4571e+00, -4.5328e-01,\n",
      "          1.6136e+00,  1.1904e+00, -4.2122e-01,  1.6349e+00,  4.5766e-01,\n",
      "          1.4562e+00, -2.4465e-01,  9.3286e-01,  1.3401e+00,  1.0039e-01,\n",
      "         -5.1096e-01,  4.4273e-01,  1.6810e+00, -4.2129e-01, -6.0141e-01,\n",
      "         -5.4790e-01, -1.7025e+00,  1.4742e+00,  1.2149e+00, -4.7192e-01,\n",
      "         -7.2275e-01,  1.7181e+00,  4.2265e-01, -1.8853e-02,  2.5416e+00,\n",
      "          6.8342e-01, -2.3677e-01,  1.0348e+00,  2.3113e-01, -1.6234e+00,\n",
      "          3.0359e-03,  5.3916e-02, -6.1735e-01, -8.1952e-01,  2.6338e+00,\n",
      "          2.9045e-01,  6.5674e-01, -1.0370e-01, -8.6304e-01,  1.2057e+00,\n",
      "         -2.0109e+00, -1.5048e+00, -1.0372e+00, -2.6365e-01,  3.7090e-01,\n",
      "          1.9918e+00, -7.6800e-01,  6.9617e-01, -5.3952e-01, -1.2247e+00,\n",
      "         -9.8275e-02, -6.6087e-01,  4.5948e-01, -2.4396e-01,  1.5077e+00,\n",
      "          3.4603e-01,  6.5973e-01,  8.2348e-01, -8.3776e-01, -2.2712e+00,\n",
      "         -1.0809e+00, -8.2309e-01,  4.5946e-01, -2.7179e-01,  7.5835e-01,\n",
      "         -2.1660e+00, -8.5583e-01, -1.5509e-01, -1.3321e+00, -5.5365e-01,\n",
      "          6.9918e-01,  8.9013e-02,  3.1330e-01,  8.4202e-01, -8.1534e-01,\n",
      "          2.0309e-01,  3.0274e-01,  1.1761e-01,  8.4229e-01, -6.6748e-01,\n",
      "         -4.6324e-01, -2.7320e-01, -1.0266e+00, -4.2591e-01, -2.7314e-01,\n",
      "         -7.7351e-01,  2.2913e-01,  1.3742e+00, -6.6027e-02, -6.7877e-01,\n",
      "         -9.5875e-02,  9.2992e-01, -1.1675e+00,  2.3320e-01,  2.2056e+00,\n",
      "         -1.6919e+00, -9.8543e-01, -9.9156e-01, -9.6392e-01,  1.0358e-01,\n",
      "          8.6465e-01, -1.4210e+00,  1.2204e+00, -6.5349e-03,  6.5847e-01,\n",
      "         -6.4357e-02,  8.4676e-01, -7.6900e-01, -5.4555e-01, -8.2971e-01,\n",
      "          8.9807e-01, -9.9507e-01, -6.2032e-01,  5.0050e-01,  2.5983e+00,\n",
      "         -1.0549e+00, -2.0377e+00, -1.7926e+00, -3.5358e-02, -7.7073e-01,\n",
      "          7.5158e-01, -6.3828e-01, -5.3700e-01,  1.0915e+00, -6.2290e-02,\n",
      "         -2.2412e+00, -1.9170e+00,  4.7426e-01, -2.0116e+00,  8.2461e-01,\n",
      "         -8.0895e-01, -2.0336e+00, -1.3167e+00,  1.2995e+00, -4.3345e-01,\n",
      "          1.3096e+00, -8.7653e-02,  2.7732e-01,  5.6644e-01,  6.1060e-01,\n",
      "          8.5302e-01,  5.3795e-01, -2.1493e-01, -8.6906e-01, -2.3746e-03,\n",
      "          8.5020e-02,  1.5472e-01, -3.0661e-02, -1.3657e+00, -1.3810e+00]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\orian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (3) to match target batch_size (5).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device)\n\u001b[0;32m      6\u001b[0m rnn \u001b[38;5;241m=\u001b[39m Rnn()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFILE_PATH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 25\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(rnn, train_ds, optimizer, dev, FILE_PATH, epoch_iter)\u001b[0m\n\u001b[0;32m     22\u001b[0m loss_func \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# ---BackPropagation---\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Use this link for the error:\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m https://discuss.pytorch.org/t/valueerror-expected-input-batch-size-324-to-match-target-batch-size-4/24498/4\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\orian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\orian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\orian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\orian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (3) to match target batch_size (5)."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.is_available() : device = \"cuda:0\"\n",
    "    else : device = \"cpu\"\n",
    "    device = torch.device(device)\n",
    "        \n",
    "    rnn = Rnn().to(device)\n",
    "    train_one_epoch(rnn, train_data, optimizer=optim.Adam(rnn.parameters(), lr=0.001), dev=device, FILE_PATH=\"model.pth\", epoch_iter=0)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
